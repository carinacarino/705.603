{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python in c:\\users\\carin\\appdata\\roaming\\python\\python312\\site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\carin\\appdata\\roaming\\python\\python312\\site-packages (from opencv-python) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T17:23:51.466694900Z",
     "start_time": "2024-03-17T17:23:50.964940Z"
    }
   },
   "id": "a570c004c71ef068"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "def load_coco_annotations(json_path):\n",
    "    \"\"\"\n",
    "    Load and parse COCO-style annotations from a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - json_path: Path to the COCO annotations JSON file.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary mapping image filenames to their corresponding bounding boxes.\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as file:\n",
    "        coco = json.load(file)\n",
    "\n",
    "    annotations = {}\n",
    "    for ann in coco['annotations']:\n",
    "        image_id = ann['image_id']\n",
    "        category_id = ann['category_id']\n",
    "        \n",
    "        # Assuming you're interested in a specific category (e.g., license plates)\n",
    "        # If category_id matches license plates, adjust this ID according to your dataset\n",
    "        if category_id == 1:  # Adjust this ID based on your COCO categories\n",
    "            bbox = ann['bbox']  # COCO bbox format is [x_min, y_min, width, height]\n",
    "            # Convert COCO bbox format to [x_min, y_min, x_max, y_max]\n",
    "            bbox = [bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]]\n",
    "            \n",
    "            # Get the corresponding image filename\n",
    "            image_info = next((item for item in coco['images'] if item[\"id\"] == image_id), None)\n",
    "            if image_info:\n",
    "                image_filename = image_info['file_name']\n",
    "                if image_filename in annotations:\n",
    "                    annotations[image_filename].append(bbox)\n",
    "                else:\n",
    "                    annotations[image_filename] = [bbox]\n",
    "\n",
    "    return annotations\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T17:23:53.374935900Z",
     "start_time": "2024-03-17T17:23:53.269045500Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "json_path = r'F:\\workspace\\705.603SP24\\license_plate_recognition\\annotations\\coco_annotations.json'\n",
    "image_dir = r'F:\\workspace\\705.603SP24\\license_plate_recognition\\license_plate_dataset'\n",
    "\n",
    "annotations = load_coco_annotations(json_path)\n",
    "\n",
    "# Prepare a list of image paths and their corresponding ground truths\n",
    "test_images = []\n",
    "ground_truths = []\n",
    "\n",
    "for image_filename, bboxes in annotations.items():\n",
    "    image_path = os.path.join(image_dir, image_filename)\n",
    "    if os.path.exists(image_path):\n",
    "        test_images.append(image_path)\n",
    "        ground_truths.append(bboxes)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T17:23:55.942796100Z",
     "start_time": "2024-03-17T17:23:55.936773600Z"
    }
   },
   "id": "d2f9e0d3f6e68406"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LPR MODEL1: YOLOV3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf16230e08506b41"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics report generated: results\\metrics_report.txt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "from model import Object_Detection_Model\n",
    "from metrics import Metrics\n",
    "\n",
    "\n",
    "lpr_model_path = (r\"F:\\workspace\\705.603SP24\\license_plate_recognition\\yolo_lpr\\lpr-yolov3.weights\",\n",
    "                  r\"F:\\workspace\\705.603SP24\\license_plate_recognition\\yolo_lpr\\lpr-yolov3.cfg\")\n",
    "\n",
    "# Initialize the model and metrics instances\n",
    "model = Object_Detection_Model(lpr_model_path)\n",
    "metrics = Metrics()\n",
    "\n",
    "# Iterate over the dataset, predicting and updating metrics\n",
    "for image_path, ground_truth in zip(test_images, ground_truths):\n",
    "    image = cv2.imread(image_path)\n",
    "    predictions = model.predict(image)\n",
    "    metrics.update(predictions, ground_truth)\n",
    "\n",
    "# Generate and display the final report\n",
    "metrics.generate_report()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T17:23:19.444208400Z",
     "start_time": "2024-03-17T17:21:41.215003900Z"
    }
   },
   "id": "ce04ce292da7107b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LPR MODEL2: YOLOV3-TINY"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "739bbf8205b149e9"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics report generated: results\\metrics_report2.txt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "from model import Object_Detection_Model\n",
    "from metrics import Metrics\n",
    "lpr_model_path2 = (r\"F:\\workspace\\705.603SP24\\license_plate_recognition\\yolo_lpr\\lpr-yolov3-tiny.weights\",\n",
    "                  r\"F:\\workspace\\705.603SP24\\license_plate_recognition\\yolo_lpr\\lpr-yolov3-tiny.cfg\")\n",
    "\n",
    "# Initialize the model and metrics instances\n",
    "model2 = Object_Detection_Model(lpr_model_path2)\n",
    "metrics = Metrics()\n",
    "\n",
    "# Iterate over the dataset, predicting and updating metrics\n",
    "for image_path, ground_truth in zip(test_images, ground_truths):\n",
    "    image = cv2.imread(image_path)\n",
    "    predictions = model2.predict(image)\n",
    "    metrics.update(predictions, ground_truth)\n",
    "\n",
    "# Generate and display the final report\n",
    "metrics.generate_report()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T17:24:14.565598900Z",
     "start_time": "2024-03-17T17:24:01.433006Z"
    }
   },
   "id": "1cc7b65423a09a3f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
